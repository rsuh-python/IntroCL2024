{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d885b9d8-7952-46d7-b2ea-50320b9cad0f",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NLTK - одна из самых первых библиотек, предназначенных для решения задач NLP; она огромная и содержит очень много разных инструментов, некоторые из них никак не связаны между собой (в отличие от современных библиотек, которые скоро посмотрим). NLTK - больше исследовательская библиотека, конструктор своего рода. Для NLTK есть учебник, написанный авторами: [NLTK book](https://www.nltk.org/book/). Для этого учебника специально существует подмодуль book, который обычно импортируется целиком:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5ec6b6-c386-4528-9e01-1fe878512225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5192a-4ac0-4042-a6c7-e36181e488c6",
   "metadata": {},
   "source": [
    "В этом модуле есть некий набор текстов и набор предложений, с которыми можно поиграться. \n",
    "\n",
    "Центральный объект для NLTK (по крайней мере, при работе с корпусами) - это Text (nltk.text.Text). По сути, в этом объекте содержится сам текст в виде списка токенов, но у него есть дополнительные методы. Что можно делать с объектом класса Text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bc1861-3616-4962-8149-16e1266fb25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 84 matches:\n",
      "[ Moby Dick by Herman Melville 1851 ] ETYMOLOGY . ( Su\n",
      "hat white whale must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye k\n",
      " must be the same that some call Moby Dick .\" \" Moby Dick ?\" shouted Ahab . \" Do ye know the white w\n",
      "ib in a squall . Death and devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \n",
      " devils ! men , it is Moby Dick ye have seen -- Moby Dick -- Moby Dick !\" \" Captain Ahab ,\" said Sta\n"
     ]
    }
   ],
   "source": [
    "text1.concordance('Moby', width=100, lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59209f-e284-48dc-9ae7-54ccbffb4a9a",
   "metadata": {},
   "source": [
    "Конкорданс ищет первые n вхождений заданного слова с шириной контекста width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1c743c-eb87-42f1-9d7c-cf5eeaebdb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ship boat sea time captain world man deck pequod other whales air\n",
      "water head crew line thing side way body\n"
     ]
    }
   ],
   "source": [
    "text1.similar('whale', num=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ea4c47-d413-40f7-ab5b-982c7dc5b0b9",
   "metadata": {},
   "source": [
    "similar возвращает num слов, которые встречаются в похожих контекстах (дистрибутивная похожесть). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7df4b93-207b-4c2c-bc8d-a34d219d4dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the_s the_and the_is the_in the_the the_as the_was the_which the_i\n",
      "a_in the_has the_when the_had the_with the_to the_by the_so the_that\n",
      "the_would the_a\n"
     ]
    }
   ],
   "source": [
    "text1.common_contexts(['whale', 'ship'])  # тоже можно задать параметр num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e93fa1f-eadf-4fd0-b6b1-3b92ea5605b9",
   "metadata": {},
   "source": [
    "common_contexts ищет те самые совпадающие контексты. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fb6f58-a776-4ef3-94ba-df8cec1d2eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text1.dispersion_plot(['Ahab', 'Ishmael'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b4f756-62c3-44fd-bba5-44ddd63444dd",
   "metadata": {},
   "source": [
    "Можно построить график распределения слов по тексту (без NumPy и matplotlib график не работает, поэтому установите эти две библиотеки, если еще не). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f11b027-62ad-4cae-985d-21c7a5527856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.count('Moby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03863563-5aa5-42b4-8555-06fb423e0727",
   "metadata": {},
   "source": [
    "Можно посчитать количество вхождений какого-то слова. Кстати, к текстам можно применять обычные функции len(), set() и подобные. И срезы с индексами работают!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedb9a4f-8747-4d59-bc77-82250d0e25a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building ngram index...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long , from one to the top - mast , and no coffin and went out a sea\n",
      "captain -- this peaking of the whales . , so as to preserve all his\n",
      "might had in former years abounding with them , they toil with their\n",
      "lances , strange tales of Southern whaling . at once the bravest\n",
      "Indians he was , after in vain strove to pierce the profundity . ?\n",
      "then ?\" a levelled flame of pale , And give no chance , watch him ;\n",
      "though the line , it is to be gainsaid . have been\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'long , from one to the top - mast , and no coffin and went out a sea\\ncaptain -- this peaking of the whales . , so as to preserve all his\\nmight had in former years abounding with them , they toil with their\\nlances , strange tales of Southern whaling . at once the bravest\\nIndians he was , after in vain strove to pierce the profundity . ?\\nthen ?\" a levelled flame of pale , And give no chance , watch him ;\\nthough the line , it is to be gainsaid . have been'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.generate(length=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac2c4e-299e-414d-8038-2c197b968386",
   "metadata": {},
   "source": [
    "Можно сгенерировать текст, \"похожий\" на оригинальный. Это делается с помощью n-грамов (или n-грамм, я видела разные варианты по-русски...): nltk просто в случайном порядке совмещает эти n-грамы. Как можно видеть, не слишком полезный метод, однако можно побаловаться. \n",
    "\n",
    "Важнее то, что в nltk есть утилиты для работы с n-грамами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "160f8dbc-9fe3-4ebe-8477-dbdb08c66729",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('THE', 'suburb', 'of'),\n",
       " ('suburb', 'of', 'Saffron'),\n",
       " ('of', 'Saffron', 'Park'),\n",
       " ('Saffron', 'Park', 'lay'),\n",
       " ('Park', 'lay', 'on'),\n",
       " ('lay', 'on', 'the'),\n",
       " ('on', 'the', 'sunset'),\n",
       " ('the', 'sunset', 'side'),\n",
       " ('sunset', 'side', 'of'),\n",
       " ('side', 'of', 'London'),\n",
       " ('of', 'London', ','),\n",
       " ('London', ',', 'as'),\n",
       " (',', 'as', 'red'),\n",
       " ('as', 'red', 'and'),\n",
       " ('red', 'and', 'ragged'),\n",
       " ('and', 'ragged', 'as'),\n",
       " ('ragged', 'as', 'a'),\n",
       " ('as', 'a', 'cloud'),\n",
       " ('a', 'cloud', 'of'),\n",
       " ('cloud', 'of', 'sunset'),\n",
       " ('of', 'sunset', '.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams # bigrams\n",
    "\n",
    "list(ngrams(sent9, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82b919-12ec-4e20-9edb-0da6d15cfeb1",
   "metadata": {},
   "source": [
    "Функция ngrams (или bigrams) возвращает список всех н-грамов списка токенов, который ей дать. N-грамы еще принимают число n. \n",
    "\n",
    "У класса Text есть метод, который возвращает коллокации (частотные н-грамы):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b105a78-b99b-417c-aa6b-e85962657a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f51b2b0-7db7-46a1-85ae-83967e0d7ef4",
   "metadata": {},
   "source": [
    "Как создать собственный объект класса Text? Достаточно токенизировать свой текст (любым токенизатором) и передать его в класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2114b22c-a2c8-4d42-8990-89448287dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc2974-f898-4759-9fad-30de73bede94",
   "metadata": {},
   "source": [
    "Гораздо чаще на практике, однако, используются какие-то отдельные инструменты NLTK, предназначенные для обработки текста. Часть мы уже знаем: это токенизаторы и сентенайзер. \n",
    "\n",
    "Следующий шаг при обработке текстов - это обычно приведение слов к словарной форме. Это уже не такая простая задача, с помощью регулярок ее не решить, приходится использовать либо словари, либо более сложные методы (нейронные сети...)\n",
    "\n",
    "Еще в прошлом веке, правда, придумали (конечно же, для английского языка) более простой способ хоть как-то унифицировать разные словоформы одного слова: стемминг. \n",
    "\n",
    "Стемминг – это уже чисто историческое, можно сказать, явление: в 1980-х, когда еще не было даже графического интерфейса у компьютеров и тем более средств автоматического морфоразбора, Мартин Портер разработал свой алгоритм стемминга: усечение окончания от псевдоосновы. Этот алгоритм так и называется \"стеммер Портера\" и доступен в версиях для нескольких европейских языков, в т.ч. для русского (Snowball – чуть более новая версия). Алгоритм с помощью правил отсекает окончания и суффиксы, основываясь на особенностях языка. Как все правиловое, работает не без ошибок.\n",
    "\n",
    "В NLTK есть несколько стеммеров, а именно:\n",
    "\n",
    "1. PorterStemmer\n",
    "2. SnowballStemmer\n",
    "3. LancasterStemmer\n",
    "4. RegexpStemmer\n",
    "5. RSLPStemmer\n",
    "\n",
    "Все смотреть не будем, можете сами поискать, если интересно, нам хватит двух. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff77802-307b-40b4-be5e-38e9fc347c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\", \"pythoner\", \"pythoning\", \"pythoned\", \"pythonly\"]\n",
    "for w in example_words:\n",
    " print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d19e91bf-be7a-4376-bc4a-a4a0b607dd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "пердикк\n",
      "не\n",
      "мен\n",
      "десят\n",
      "раз\n",
      "заключа\n",
      "и\n",
      "расторга\n",
      "союз\n",
      "с\n",
      "основн\n",
      "участник\n",
      "войн\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('russian')  # экземпляр класса \n",
    "example = ['Пердикка', 'не', 'менее', 'десяти', 'раз', 'заключал', 'и', 'расторгал', 'союзы', 'с', 'основными', 'участниками', 'войны', '.']\n",
    "for token in example:\n",
    "    print(stemmer.stem(token))  # stem() - метод класса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9fb23-a579-4cb3-a3d2-4b3541cda5fa",
   "metadata": {},
   "source": [
    "Другой, более крутой способ унифицировать словоформы - все-таки приводить их к словарной форме. В NLTK есть WordNetLemmatizer, который ищет нужные леммы в словаре. Работает только для английского. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ef6d0ba-9083-4a5b-81a4-3b2a89e6abec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "play\n",
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n",
      "cat\n",
      "cactus\n",
      "goose\n",
      "rock\n",
      "python\n",
      "good\n",
      "best\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('increases'))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"run\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7b640-55d2-41a6-b966-f6236cdc154c",
   "metadata": {},
   "source": [
    "При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e778a4c5-b6c4-4103-ad5c-5a5751676a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стоп-слова: {'зачем', 'бы', 'сам', 'другой', 'есть', 'была', 'много', 'под', 'чуть', 'да', 'при', 'между', 'всегда', 'же', 'сейчас', 'ним', 'тем', 'а', 'по', 'ну', 'эти', 'с', 'было', 'от', 'ей', 'будто', 'один', 'тебя', 'этом', 'какая', 'и', 'через', 'чтоб', 'три', 'вдруг', 'к', 'тот', 'совсем', 'был', 'разве', 'него', 'для', 'хорошо', 'они', 'вас', 'том', 'потому', 'над', 'нельзя', 'быть', 'этот', 'со', 'себе', 'опять', 'всего', 'мне', 'какой', 'ее', 'ничего', 'перед', 'где', 'на', 'когда', 'более', 'меня', 'без', 'о', 'кто', 'мы', 'нас', 'может', 'куда', 'ему', 'эту', 'моя', 'что', 'за', 'лучше', 'чтобы', 'можно', 'им', 'ж', 'во', 'его', 'того', 'этой', 'конечно', 'такой', 'чего', 'них', 'наконец', 'в', 'уж', 'здесь', 'иногда', 'ней', 'вы', 'два', 'еще', 'там', 'после', 'нибудь', 'не', 'из', 'нет', 'ли', 'если', 'никогда', 'этого', 'больше', 'были', 'теперь', 'про', 'до', 'почти', 'раз', 'я', 'так', 'но', 'ни', 'тогда', 'или', 'себя', 'все', 'мой', 'она', 'ведь', 'вот', 'только', 'хоть', 'чем', 'ты', 'об', 'нее', 'свою', 'их', 'он', 'вам', 'тут', 'надо', 'потом', 'у', 'как', 'будет', 'то', 'даже', 'всех', 'всю', 'уже', 'впрочем', 'тоже'}\n",
      "['обработке', 'текстов', 'решения', 'задач', 'NLP', ',', 'особенно', 'используются', 'классические', 'алгоритмы', 'машинного', 'обучения', '(', 'современные', 'нейронки', 'извлекают', 'признаки', ',', 'часто', ',', 'текст', 'сохранялся', 'исходном', 'виде', ')', ',', 'бывает', 'нужно', 'выкинуть', 'слишком', 'распространенные', 'малозначимые', 'слова', ':', 'союзы', ',', 'предлоги', 'т.п', '.', 'NLTK', 'списки', 'таких', 'слов', 'разных', 'языков']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "example_sent = 'При обработке текстов для решения задач NLP, особенно если используются классические алгоритмы машинного обучения (современные нейронки извлекают признаки из всего, им часто лучше, чтобы текст сохранялся в исходном виде), бывает нужно выкинуть слишком распространенные и малозначимые слова: союзы, предлоги и т.п. В NLTK есть списки таких слов для разных языков'\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "print('Стоп-слова:', stop_words)\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6e60b-d487-4d8b-9508-3e738f47a4da",
   "metadata": {},
   "source": [
    "Наконец к морфологии. Подробнее о ней мы поговорим в следующие разы, потому что морфологический разбор (с частями речи и еще и грамматическими характеристиками) - еще более сложная задача. В NLTK реализованы простейшие статистические парсеры, которые можно самостоятельно обучать на данных размеченных корпусов NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7860d712-f7fa-4447-8dea-fb925ee4b1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('And', 'CC'), ('now', 'RB'), ('for', 'IN'), ('something', 'NN'), ('completely', 'RB'), ('different', 'JJ')]\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = word_tokenize(\"And now for something completely different\")\n",
    "print(nltk.pos_tag(text))\n",
    "nltk.help.upenn_tagset('NN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "623758d8-01e1-4408-a7b4-26428b769408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', 'NNS'),\n",
       " ('with', 'IN'),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown # Брауновский корпус\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news') # возьмем все размеченные предложения из новостей\n",
    "unigram_tagger = nltk.UnigramTagger(brown_tagged_sents) # натренируем таггер на размеченных предложениях (он просто посчитает статистику)\n",
    "unigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK')) # попробуем на предложении, которого он не видел"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e17494a3-86d8-4172-af7d-7fe4fb77df51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PPSS'),\n",
       " ('want', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('analyze', None),\n",
       " ('sentences', None),\n",
       " ('with', None),\n",
       " ('NLTK', None)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = nltk.BigramTagger(brown_tagged_sents)\n",
    "bigram_tagger.tag(word_tokenize('I want to analyze sentences with NLTK'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62fcc9-2617-44df-96ec-fafcf57bdfc1",
   "metadata": {},
   "source": [
    "Bigram tagger работает как будто бы хуже, чем unigram, но в теории он может лучше справляться с омонимией (потому что учитывает рядом стоящее слово, и тогда beautiful book vs book something не разберет одинаково). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f36de76-28be-444a-b7cf-5ccc09d493a5",
   "metadata": {},
   "source": [
    "Также NLTK умеет работать с данными базы [WordNet](https://wordnet.princeton.edu/). Оттуда он автоматически может извлекать сведения о семантических отношениях между словами, а также на данных WordNet у него реализован алгоритм Леска для решения задачи Word Sense Disambiguation. \n",
    "\n",
    "Это очень известная задача в NLP-мире, можно про нее подробнее посмотреть на [nlpprogress](http://nlpprogress.com/english/word_sense_disambiguation.html). Для ее решения мы должны неоднозначным словам в контекстах сопоставить дефиниции из словаря (в роли какового для английского языка успешно выступает WordNet). Алгоритм Леска был придуман в 1986 году и считается классическим подходом (бейзлайн, ага) для решения этой задачи. Мы предполагаем, что слова в заданном окне контекста (среди окружающих их слов) будут иметь похожую тематику. Это еще называется **дистрибутивная гипотеза** (\"Лингвистические единицы, встречающиеся в схожих контекстах, имеют близкие значения.\", придумали это лингвисты уже много лет назад). По алгоритму Леска, определение в словаре для целевого слова сравнивается со словами, которые стоят вокруг него в контексте. \n",
    "\n",
    "В базовой имплементации алгоритм Леска делает следующее:\n",
    "\n",
    "- считает количество слов, стоящих рядом с искомым словом и оказавшихся в словарном определении слова (для каждого варианта определения)\n",
    "- Каких слов больше всего оказалось, то и значение. \n",
    "\n",
    "Очень просто!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b2a8817-9428-4295-a49a-7a326c4da0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "def get_semantic(seq, key_word):\n",
    "    temp = word_tokenize(seq)\n",
    "    temp = lesk(temp, key_word)\n",
    "    return temp.definition() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93507a61-9a02-4a73-a4a1-da8b03794de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arrange for and reserve (something for someone else) in advance\n",
      "a number of sheets (ticket or stamps etc.) bound together on one edge\n"
     ]
    }
   ],
   "source": [
    "print(get_semantic('The table was already booked by someone else', 'book'))\n",
    "print(get_semantic('I love reading books on programming', 'book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb628a77-dd06-4307-aac5-5ec2909e90bd",
   "metadata": {},
   "source": [
    "Ну и синонимы с антонимами просто собираются напрямую из WordNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "250651dd-432e-455d-b890-269d6795ad3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synonyms ['dog.n.01', 'frump.n.01', 'dog.n.03', 'cad.n.01', 'frank.n.02', 'pawl.n.01', 'andiron.n.01', 'chase.v.01']\n",
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synonyms = []\n",
    "for syns in wordnet.synsets('dog'):\n",
    " synonyms.append(syns.name())\n",
    "print (\"synonyms\", synonyms)\n",
    "\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    " for l in syn.lemmas():\n",
    "  if l.antonyms():\n",
    "   antonyms.append(l.antonyms()[0].name())\n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90f7de-3a7e-47b9-bd99-3081c54d10be",
   "metadata": {},
   "source": [
    "## Правиловые морфопарсеры русского языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979b2f0b-15d2-49ab-98b6-e0bca19cfad7",
   "metadata": {},
   "source": [
    "Установка нужных для него библиотек:\n",
    "\n",
    "**pymorphy2**\n",
    "\n",
    "*pip install pymorphy2*\n",
    "\n",
    "*pip install -U pymorphy2-dicts-ru*  # необязательно: для обновления словаря\n",
    "\n",
    "*Colab*: устанавливается без проблем. \n",
    "\n",
    "**Mystem**\n",
    "\n",
    "*pip install git+https://github.com/nlpub/pymystem3* \n",
    "\n",
    "(возможно, уже перестало работать для новых версий питона: можно установить просто как pip install pymystem3, но тогда лучше не запускать на нем длинные тексты с переносами на новую строку.)\n",
    "\n",
    "*Colab*: гарантированно не работает. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944aba64-948f-4f7c-aa15-74b219a40010",
   "metadata": {},
   "source": [
    "Почти все нижеописанные инструменты в питоне устроены довольно однообразно: имеется основной класс \"парсер\", экземпляр которого вам нужно создать, прежде чем что-то парсить. То есть, условно говоря, из набора машинок берете ту конкретную, которая вас повезет. Когда создан экземпляр класса, ему уже можно скармливать свои тексты. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc183d-7ea0-4864-8d72-c31106a44026",
   "metadata": {},
   "source": [
    "Самые простые &ndash; правиловые морфопарсеры. Для русского языка их два: pymorphy2 и pymystem3. Pymorphy был создан Михаилом Коробовым (вот его известная [статья на хабре](https://habr.com/ru/post/176575/)) как аналог Майстем. Он работает на словаре и использует тагсет [OpenCorpora](http://opencorpora.org/)), а также статистику, предпосчитанную на этом корпусе. \n",
    "Как устроен pymorphy2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c4696bd-861b-4915-91d0-9436b42d2fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn sing,gent'), normal_form='студентка', score=0.6, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 1),)),\n",
       " Parse(word='студентки', tag=OpencorporaTag('NOUN,anim,femn plur,nomn'), normal_form='студентка', score=0.4, methods_stack=((DictionaryAnalyzer(), 'студентки', 40, 7),))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "parse = morph.parse('студентки')\n",
    "parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df7acb-e5ed-4cba-a99a-a8fc6c47989e",
   "metadata": {},
   "source": [
    "У класса MorphAnalyzer() есть метод parse, который возвращает что? Список экземпляров класса Parse. У этого класса есть свои атрибуты: word (исходная форма слова), tag (грам. инфа), normal_form (лемма), score(предпосчитанная на OpenCorpora вероятность правильности разбора) и несколько технических. \n",
    "\n",
    "Соответственно, получить информацию можно, просто обращаясь к атрибутам (не забудьте, что у нас всегда список, поэтому нужно еще и индекс разбора указывать):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "957d5732-ccf0-4048-a9a8-e021f53f5299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "студентки\n",
      "NOUN,anim,femn sing,gent\n",
      "студентка\n"
     ]
    }
   ],
   "source": [
    "print(parse[0].word)\n",
    "print(parse[0].tag)\n",
    "print(parse[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6eb23-1bd5-46e2-b2fe-2fd05e3224a2",
   "metadata": {},
   "source": [
    "Атрибут tag &ndash; это экземпляр класса OpencorporaTag, как можно догадаться. У него есть еще свои атрибуты, к которым тоже можно обращаться, чтобы получать более конкретную информацию о слове. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94a49e9f-45ff-45c1-bd7e-697678e1a85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: NOUN\n",
      "Одушевленность: anim\n",
      "Падеж: nomn\n",
      "Род: masc\n",
      "Наклонение: None\n",
      "Число: sing\n",
      "Лицо: None\n",
      "Время: None\n",
      "Переходность: None\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('участник')\n",
    "\n",
    "t = parse[0].tag  # я записала в переменную, просто чтобы не копировать каждый раз все целиком\n",
    "# но это то же самое, что parse[0].tag.animacy...\n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\nНаклонение: {t.mood}\\\n",
    "\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd14aaaa-f056-4e80-bb79-771e927edf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Часть речи: VERB\n",
      "Одушевленность: None\n",
      "Падеж: None\n",
      "Род: None\n",
      "Наклонение: indc\n",
      "Число: sing\n",
      "Лицо: 3per\n",
      "Время: pres\n",
      "Переходность: tran\n",
      "Залог: None\n"
     ]
    }
   ],
   "source": [
    "parse = morph.parse('говорит')\n",
    "\n",
    "t = parse[0].tag  \n",
    "print(f'Часть речи: {t.POS}')\n",
    "print(f'Одушевленность: {t.animacy}\\nПадеж: {t.case}\\nРод: {t.gender}\\n\\\n",
    "Наклонение: {t.mood}\\nЧисло: {t.number}\\nЛицо: {t.person}\\nВремя: {t.tense}\\nПереходность: {t.transitivity}\\nЗалог: {t.voice}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff502f-7293-4da5-8962-567245591a97",
   "metadata": {},
   "source": [
    "Если вы запрашиваете категорию, которой у данного слова нет (ну нет переходности у существительного), вернется None. \n",
    "\n",
    "Также можно попросить pymorphy поставить слово в конкретную форму или вообще вернуть всю парадигму. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a7685c-97b8-47fe-94d1-c85845034961",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='говорят', tag=OpencorporaTag('VERB,impf,tran plur,3per,pres,indc'), normal_form='говорить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'говорят', 415, 6),))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].inflect({'plur'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "006f4432-cc3a-4a4b-86cf-b2752aa89fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='участник', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участник', 2, 0),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 1),)),\n",
       " Parse(word='участнику', tag=OpencorporaTag('NOUN,anim,masc sing,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнику', 2, 2),)),\n",
       " Parse(word='участника', tag=OpencorporaTag('NOUN,anim,masc sing,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участника', 2, 3),)),\n",
       " Parse(word='участником', tag=OpencorporaTag('NOUN,anim,masc sing,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участником', 2, 4),)),\n",
       " Parse(word='участнике', tag=OpencorporaTag('NOUN,anim,masc sing,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участнике', 2, 5),)),\n",
       " Parse(word='участники', tag=OpencorporaTag('NOUN,anim,masc plur,nomn'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участники', 2, 6),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,gent'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 7),)),\n",
       " Parse(word='участникам', tag=OpencorporaTag('NOUN,anim,masc plur,datv'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участникам', 2, 8),)),\n",
       " Parse(word='участников', tag=OpencorporaTag('NOUN,anim,masc plur,accs'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участников', 2, 9),)),\n",
       " Parse(word='участниками', tag=OpencorporaTag('NOUN,anim,masc plur,ablt'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниками', 2, 10),)),\n",
       " Parse(word='участниках', tag=OpencorporaTag('NOUN,anim,masc plur,loct'), normal_form='участник', score=1.0, methods_stack=((DictionaryAnalyzer(), 'участниках', 2, 11),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].lexeme  \n",
    "# парадигму глагола лучше не выводить - она длинная; я перед запуском этой ячейки перезапустила разбор с существительным, поэтому не удивляйтесь. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf612f50-e3f3-4cd1-a4d7-bac2217b1c4e",
   "metadata": {},
   "source": [
    "Наконец, можно попросить pymorphy выводить грам. информацию по-русски:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e06b7e2c-a339-4d5e-9dcf-4d7468d9deb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'СУЩ,од,мр ед,им'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0].tag.cyr_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e41e41-988f-4745-83e9-71c046832348",
   "metadata": {},
   "source": [
    "Pymorphy очень быстро работает и имеет много возможностей, но совершенно не умеет разрешать омонимию и никак не учитывает контекст."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c85f6-ee09-410e-b1cd-f9e1df71374e",
   "metadata": {},
   "source": [
    "Алгоритм, легший в основу Mystem, разрабатывался в ИППИ и был первым вообще для русского языка; его в свое время купил у них Илья Сегалович, доработал, опубликовал собственную статью. Поисковик Яндекса когда-то работал на майстеме. Сам парсер написан в С (для скорости: бинарный поиск в питоне реализовать можно только с внешними библиотеками на С, а у майстема 2 словаря, по которым нужно искать). Для питона под него сделана оболочка (pymystem3). Майстем капризный, тяжело запускается, имеет не так много функций, но работает тоже довольно быстро и умеет доносить на бастардов: сообщать, что слово не найдено в его словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5267089e-5654-48ba-89b1-b4adb7dafbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "\n",
    "m = pymystem3.Mystem(entire_input=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc52f6ee-cb1f-4ed7-b33d-87e890e62a97",
   "metadata": {},
   "source": [
    "Майстем принимает только сырой текст в виде одной строки: у него встроенный токенизатор, потому что он пытается учитывать контекст. Поэтому стоит указывать entire_input=False при создании экземпляра класса, чтобы он не выводил вообще все, включая пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c0e205a-cdec-4371-8382-0327f7cd9cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = '''Пердикка II (др.-греч. Περδίκκας Β΄ της Μακεδονίας) — македонский царь, правивший в 454—413 годах до н. э. После смерти Александра I среди его сыновей возник междоусобный конфликт, победителем из которого вышел Пердикка. На момент его воцарения Македония представляла собой отсталое государство, которому угрожала опасность завоевания как со стороны Афинского морского союза на юге, так и Одрисского царства на севере. На первых порах Пердикка был вынужден всеми силами избегать открытого вооружённого противостояния и лишь наблюдать за появлением множества греческих колоний на своих границах. С началом Пелопоннесской войны македонский царь с максимальной выгодой для государства использовал запутанные отношения между греческими полисами на Халкидиках, Афинами, Спартой и Коринфом. Пердикка не менее десяти раз заключал и расторгал союзы с основными участниками войны.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6961724c-18f0-46df-ad28-0a07cd20152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = m.lemmatize(raw)\n",
    "analysis = m.analyze(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "117b1596-14d6-4d66-9c3d-7685314d3e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пердикк',\n",
       " 'II',\n",
       " 'др',\n",
       " 'греча',\n",
       " 'Περδίκκας',\n",
       " 'Β',\n",
       " 'της',\n",
       " 'Μακεδονίας',\n",
       " 'македонский',\n",
       " 'царь']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[:10]  # надеюсь, греча вас тоже порадовала"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08fb55cb-e7c8-4658-b291-e96043df7b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'analysis': [{'lex': 'пердикк',\n",
       "    'qual': 'bastard',\n",
       "    'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}],\n",
       "  'text': 'Пердикка'},\n",
       " {'analysis': [], 'text': 'II'},\n",
       " {'analysis': [{'lex': 'др', 'gr': 'S,сокр,мн,неод=(пр|вин|дат|род|твор|им)'}],\n",
       "  'text': 'др'},\n",
       " {'analysis': [{'lex': 'греча', 'gr': 'S,жен,неод=род,мн'}], 'text': 'греч'},\n",
       " {'analysis': [], 'text': 'Περδίκκας'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0461c82-d4ef-4f09-a7b0-462eae195a58",
   "metadata": {},
   "source": [
    "С леммами вроде все должно быть понятно, а что зашито в анализе?\n",
    "\n",
    "Майстем возвращает список. Каждый токен в этом списке - это словарь с ключами analysis & text. Первого ключа может не быть: если у нас знак пунктуации. Если же он есть, то в нем содержится список (обычно состоящий из одного элемента - если не указать при создании экземпляра класса Mystem glue_grammar_info=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "effb14ea-3928-49e7-8ca3-f3f6cc59c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первое слово: {'analysis': [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}], 'text': 'Пердикка'}\n",
      "Его грам. инфа: [{'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=(вин,ед|род,ед)'}]\n",
      "Его оригинальная форма: Пердикка\n",
      "Какие есть ключи в словаре с разбором: dict_keys(['lex', 'qual', 'gr'])\n",
      "Лемма: пердикк\n",
      "Этот ключ бывает только тогда, когда слова нет в словаре: bastard\n",
      "А это грам. информация: S,имя,муж,од=(вин,ед|род,ед)\n"
     ]
    }
   ],
   "source": [
    "print(f'Первое слово: {analysis[0]}')\n",
    "print(f\"Его грам. инфа: {analysis[0]['analysis']}\\nЕго оригинальная форма: {analysis[0]['text']}\")\n",
    "print(f\"Какие есть ключи в словаре с разбором: {analysis[0]['analysis'][0].keys()}\")\n",
    "print(f\"Лемма: {analysis[0]['analysis'][0]['lex']}\\n\\\n",
    "Этот ключ бывает только тогда, когда слова нет в словаре: {analysis[0]['analysis'][0]['qual']}\\n\\\n",
    "А это грам. информация: {analysis[0]['analysis'][0]['gr']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6b985-d7ee-4894-aa8f-099e6b96f33c",
   "metadata": {},
   "source": [
    "Непросто, да. Еще сложнее устроен ключ 'gr', который содержит грамматическую информацию о слове: обычно майстем склеивает варианты разбора, то есть, выше запись следует читать как \"существительное, имя собственное, мужского рода, одушевленное; возможно, Acc Sg, а возможно, Gen Sg. \n",
    "\n",
    "Вот как раз если указать, чтобы грам. информация не склеивалась, майстем будет возвращать несколько словарей с вариантами по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc694849-17a4-4080-9f25-cd4e99c5f635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysis': [{'lex': 'пердикк',\n",
       "   'qual': 'bastard',\n",
       "   'gr': 'S,имя,муж,од=вин,ед'},\n",
       "  {'lex': 'пердикк', 'qual': 'bastard', 'gr': 'S,имя,муж,од=род,ед'}],\n",
       " 'text': 'Пердикка'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_noglue = pymystem3.Mystem(entire_input=False, glue_grammar_info=False)\n",
    "\n",
    "noglueanalysis = m_noglue.analyze(raw)\n",
    "noglueanalysis[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58940b30-0bdf-4a91-9786-1ac2486cfba2",
   "metadata": {},
   "source": [
    "Теперь о вещах, которых нет в стабильной версии Mystem, а есть только в той, которая устанавливается через git:\n",
    "\n",
    "1. Майстем очень плохо умеет обрабатывать \\n. Когда он получает строку, в которой много \\n (а это неизбежно, ведь мы чаще хотим обрабатывать длиннющие тексты), на каждом \\n он перезапускает свой бинарник (написанный в С). Поэтому на длинных текстах работать будет ОЧЕНЬ медленно (впрочем, все равно быстрее нейронок...). Чтобы решить эту проблему - ведь замена \\n на пробелы, например, искажает контекст - сделали возможность особым образом обрабатывать \\n, когда загружаем текст из файла. \n",
    "2. Есть функция, которая позволяет получить часть речи для конкретного токена. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd0a3a8-5a7a-46b5-8c1c-b88414e6611c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = m.analyze(file_path=path) # можно напрямую передавать в майстем путь к файлу с текстом - он сам откроет и обработает как ему надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e8ae18-325f-4c55-b224-c97451526566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.get_pos(analysis[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
