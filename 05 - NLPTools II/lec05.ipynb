{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96018b27-cc04-41a6-955a-452b43f34e2d",
   "metadata": {},
   "source": [
    "# Universal Dependencies и парсинг в этом формате"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dabbd7-1efa-4b5d-8141-61034e1126ce",
   "metadata": {},
   "source": [
    "[Universal Dependencies](https://universaldependencies.org/) - это грандиозный существующий с 2006 года проект (сперва чешских, а потом и самых разных лингвистов, у нас в России им активно занимается О. Ляшевская), который ставит целью разработать такой формат морфосинтаксической разметки, который был бы одинаково применим к самым разным языкам. То есть, основная его фича - это *единообразие*, из-за чего, к примеру, принято решение в русском языке частицу \"не\" считать advmod (так она себя ведет в германских языках...), а копулу не считать вершиной (потому что копула в агглютинативных языках обычно отсутствует, ср. русское \"Петя был учителем\" vs турецкое \"Petya öğretmendi\", где -di - показатель прошедшего времени, присоединяющийся к *существительному*). \n",
    "\n",
    "UD для разметки использует формат файлов .conllu, которые представляют собой таблички. В этом формате существует 10 колонок, каждая ячейка в строке отделяется знаком табуляции; предложения разделяются пустой строкой. На самом сайте UD очень много полезной информации, в том числе описание этого формата и сборник ссылок на приложения, которыми его удобно читать!\n",
    "\n",
    "В питоне существует как минимум две библиотеки, которые позволяют работать с этим форматом: conllu и pyconll.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe5bf7-ed67-470e-9bad-1f9857dcd5d9",
   "metadata": {},
   "source": [
    "pyconll выглядит следующим образом:\n",
    "\n",
    "    pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8ee4b-5702-4ad3-958b-5595c1cc7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "\n",
    "text = pyconll.load_from_file('myfile.conllu')\n",
    "\n",
    "for sentence in text:\n",
    "    for token in sentence:\n",
    "        print(token.id, token.form, token.lemma, token.upos, token.feats, token.head, token.deprel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d4856-3b3b-4e05-9809-03df7a99cdf3",
   "metadata": {},
   "source": [
    "Естественно, можно не только печатать информацию, но и добавлять в список и вообще делать все, что угодно. Что это за атрибуты у токенов?\n",
    "\n",
    "- id - порядковый номер токена в предложении\n",
    "- form - исходная форма\n",
    "- lemma - лемма\n",
    "- upos - часть речи в UD\n",
    "- xpos - часть речи в неуниверсальном формате (обычно встречается, если датасет конвертированный)\n",
    "- feats - грам. характеристики\n",
    "- head - расстояние от синтаксической вершины\n",
    "- deprel - тип синтаксической связи\n",
    "- две зарезервированные ячейки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf7071e-232d-4162-a152-6782c232a335",
   "metadata": {},
   "source": [
    "conllu тоже устроен очень просто:\n",
    "\n",
    "    pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f013aacc-2b42-405d-bd52-10b7812ac711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman wrote in a blog post Monday.\n",
      "1\t“\t“\tPUNCT\t``\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "2\tWhile\twhile\tSCONJ\tIN\tNone\t9\tmark\t[('mark', 9)]\tNone\n",
      "3\tmuch\tmuch\tADJ\tJJ\t{'Degree': 'Pos'}\t9\tnsubj\t[('nsubj', 9)]\tNone\n",
      "4\tof\tof\tADP\tIN\tNone\t7\tcase\t[('case', 7)]\tNone\n",
      "5\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t7\tdet\t[('det', 7)]\tNone\n",
      "6\tdigital\tdigital\tADJ\tJJ\t{'Degree': 'Pos'}\t7\tamod\t[('amod', 7)]\tNone\n",
      "7\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t3\tnmod\t[('nmod:of', 3)]\tNone\n",
      "8\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t9\tcop\t[('cop', 9)]\tNone\n",
      "9\tunprecedented\tunprecedented\tADJ\tJJ\t{'Degree': 'Pos'}\t20\tadvcl\t[('advcl:while', 20)]\tNone\n",
      "10\tin\tin\tADP\tIN\tNone\t13\tcase\t[('case', 13)]\tNone\n",
      "11\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t13\tdet\t[('det', 13)]\tNone\n",
      "12\tUnited\tUnited\tPROPN\tNNP\t{'Number': 'Sing'}\t13\tcompound\t[('compound', 13)]\tNone\n",
      "13\tStates\tStates\tPROPN\tNNPS\t{'Number': 'Plur'}\t9\tobl\t[('obl:in', 9)]\t{'SpaceAfter': 'No'}\n",
      "14\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "15\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t17\tdet\t[('det', 17)]\tNone\n",
      "16\tpeaceful\tpeaceful\tADJ\tJJ\t{'Degree': 'Pos'}\t17\tamod\t[('amod', 17)]\tNone\n",
      "17\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t20\tnsubj\t[('nsubj', 20)]\tNone\n",
      "18\tof\tof\tADP\tIN\tNone\t19\tcase\t[('case', 19)]\tNone\n",
      "19\tpower\tpower\tNOUN\tNN\t{'Number': 'Sing'}\t17\tnmod\t[('nmod:of', 17)]\tNone\n",
      "20\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t0\troot\t[('root', 0)]\tNone\n",
      "21\tnot\tnot\tADV\tRB\t{'Polarity': 'Neg'}\t20\tadvmod\t[('advmod', 20)]\t{'SpaceAfter': 'No'}\n",
      "22\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "23\t”\t”\tPUNCT\t''\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "24\tObama\tObama\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tcompound\t[('compound', 26)]\tNone\n",
      "25\tspecial\tspecial\tADJ\tJJ\t{'Degree': 'Pos'}\t26\tamod\t[('amod', 26)]\tNone\n",
      "26\tassistant\tassistant\tNOUN\tNN\t{'Number': 'Sing'}\t29\tnsubj\t[('nsubj', 29)]\tNone\n",
      "27\tKori\tKori\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "28\tSchulman\tSchulman\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "29\twrote\twrite\tVERB\tVBD\t{'Mood': 'Ind', 'Tense': 'Past', 'VerbForm': 'Fin'}\t20\tparataxis\t[('parataxis', 20)]\tNone\n",
      "30\tin\tin\tADP\tIN\tNone\t33\tcase\t[('case', 33)]\tNone\n",
      "31\ta\ta\tDET\tDT\t{'Definite': 'Ind', 'PronType': 'Art'}\t33\tdet\t[('det', 33)]\tNone\n",
      "32\tblog\tblog\tNOUN\tNN\t{'Number': 'Sing'}\t33\tcompound\t[('compound', 33)]\tNone\n",
      "33\tpost\tpost\tNOUN\tNN\t{'Number': 'Sing'}\t29\tobl\t[('obl:in', 29)]\tNone\n",
      "34\tMonday\tMonday\tPROPN\tNNP\t{'Number': 'Sing'}\t29\tnmod:tmod\t[('nmod:tmod', 29)]\t{'SpaceAfter': 'No'}\n",
      "35\t.\t.\tPUNCT\t.\tNone\t20\tpunct\t[('punct', 20)]\tNone\n"
     ]
    }
   ],
   "source": [
    "from conllu import parse # парсит одиночное предложение, загружает все в оперативную память\n",
    "from conllu import parse_incr # загружает в память предложения по очереди\n",
    "\n",
    "with open(\"en_pud-ud-test.conllu\", \"r\", encoding=\"utf-8\") as datafile:\n",
    "    data = datafile.read()\n",
    "    sentences = parse(data) # data - строка, содержащая разметку; возвращает специальный объект, у которого есть список токенов и исходный текст\n",
    "print(sentences[0].metadata['text']) # в атрибуте metadata содержатся метаданные о предложении: его sent_id, text, возможно, перевод (если есть)\n",
    "for token in sentences[0]:\n",
    "    print(token['id'], token['form'], token['lemma'], token['upos'], token['xpos'], token['feats'], token['head'], token['deprel'], token['deps'], token['misc'], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6059f2c8-4c01-4359-a304-5898a0ce3257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“While much of the digital transition is unprecedented in the United States, the peaceful transition of power is not,” Obama special assistant Kori Schulman wrote in a blog post Monday.\n",
      "1\t“\t“\tPUNCT\t``\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "2\tWhile\twhile\tSCONJ\tIN\tNone\t9\tmark\t[('mark', 9)]\tNone\n",
      "3\tmuch\tmuch\tADJ\tJJ\t{'Degree': 'Pos'}\t9\tnsubj\t[('nsubj', 9)]\tNone\n",
      "4\tof\tof\tADP\tIN\tNone\t7\tcase\t[('case', 7)]\tNone\n",
      "5\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t7\tdet\t[('det', 7)]\tNone\n",
      "6\tdigital\tdigital\tADJ\tJJ\t{'Degree': 'Pos'}\t7\tamod\t[('amod', 7)]\tNone\n",
      "7\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t3\tnmod\t[('nmod:of', 3)]\tNone\n",
      "8\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t9\tcop\t[('cop', 9)]\tNone\n",
      "9\tunprecedented\tunprecedented\tADJ\tJJ\t{'Degree': 'Pos'}\t20\tadvcl\t[('advcl:while', 20)]\tNone\n",
      "10\tin\tin\tADP\tIN\tNone\t13\tcase\t[('case', 13)]\tNone\n",
      "11\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t13\tdet\t[('det', 13)]\tNone\n",
      "12\tUnited\tUnited\tPROPN\tNNP\t{'Number': 'Sing'}\t13\tcompound\t[('compound', 13)]\tNone\n",
      "13\tStates\tStates\tPROPN\tNNPS\t{'Number': 'Plur'}\t9\tobl\t[('obl:in', 9)]\t{'SpaceAfter': 'No'}\n",
      "14\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "15\tthe\tthe\tDET\tDT\t{'Definite': 'Def', 'PronType': 'Art'}\t17\tdet\t[('det', 17)]\tNone\n",
      "16\tpeaceful\tpeaceful\tADJ\tJJ\t{'Degree': 'Pos'}\t17\tamod\t[('amod', 17)]\tNone\n",
      "17\ttransition\ttransition\tNOUN\tNN\t{'Number': 'Sing'}\t20\tnsubj\t[('nsubj', 20)]\tNone\n",
      "18\tof\tof\tADP\tIN\tNone\t19\tcase\t[('case', 19)]\tNone\n",
      "19\tpower\tpower\tNOUN\tNN\t{'Number': 'Sing'}\t17\tnmod\t[('nmod:of', 17)]\tNone\n",
      "20\tis\tbe\tAUX\tVBZ\t{'Mood': 'Ind', 'Number': 'Sing', 'Person': '3', 'Tense': 'Pres', 'VerbForm': 'Fin'}\t0\troot\t[('root', 0)]\tNone\n",
      "21\tnot\tnot\tADV\tRB\t{'Polarity': 'Neg'}\t20\tadvmod\t[('advmod', 20)]\t{'SpaceAfter': 'No'}\n",
      "22\t,\t,\tPUNCT\t,\tNone\t20\tpunct\t[('punct', 20)]\t{'SpaceAfter': 'No'}\n",
      "23\t”\t”\tPUNCT\t''\tNone\t20\tpunct\t[('punct', 20)]\tNone\n",
      "24\tObama\tObama\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tcompound\t[('compound', 26)]\tNone\n",
      "25\tspecial\tspecial\tADJ\tJJ\t{'Degree': 'Pos'}\t26\tamod\t[('amod', 26)]\tNone\n",
      "26\tassistant\tassistant\tNOUN\tNN\t{'Number': 'Sing'}\t29\tnsubj\t[('nsubj', 29)]\tNone\n",
      "27\tKori\tKori\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "28\tSchulman\tSchulman\tPROPN\tNNP\t{'Number': 'Sing'}\t26\tflat\t[('flat', 26)]\tNone\n",
      "29\twrote\twrite\tVERB\tVBD\t{'Mood': 'Ind', 'Tense': 'Past', 'VerbForm': 'Fin'}\t20\tparataxis\t[('parataxis', 20)]\tNone\n",
      "30\tin\tin\tADP\tIN\tNone\t33\tcase\t[('case', 33)]\tNone\n",
      "31\ta\ta\tDET\tDT\t{'Definite': 'Ind', 'PronType': 'Art'}\t33\tdet\t[('det', 33)]\tNone\n",
      "32\tblog\tblog\tNOUN\tNN\t{'Number': 'Sing'}\t33\tcompound\t[('compound', 33)]\tNone\n",
      "33\tpost\tpost\tNOUN\tNN\t{'Number': 'Sing'}\t29\tobl\t[('obl:in', 29)]\tNone\n",
      "34\tMonday\tMonday\tPROPN\tNNP\t{'Number': 'Sing'}\t29\tnmod:tmod\t[('nmod:tmod', 29)]\t{'SpaceAfter': 'No'}\n",
      "35\t.\t.\tPUNCT\t.\tNone\t20\tpunct\t[('punct', 20)]\tNone\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"en_pud-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "for i, sentence in enumerate(parse_incr(data_file)):\n",
    "    if i != 0:\n",
    "        break\n",
    "    print(sentence.metadata['text'])\n",
    "    for token in sentence:\n",
    "        print(token['id'], token['form'], token['lemma'], token['upos'], token['xpos'], token['feats'], token['head'], token['deprel'], token['deps'], token['misc'], sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49edec21-7d73-48f9-ab60-368042c3b912",
   "metadata": {},
   "source": [
    "Где можно красивенько отрисовать .conllu файлы в виде деревьев зависимости:\n",
    "\n",
    "[Арборатор](https://arborator.ilpga.fr/q.cgi): достаточно вставить текст в формате .conllu\n",
    "\n",
    "[Conllu-Viewer на сайте UD](https://universaldependencies.org/conllu_viewer.html): умеет читать файлы и рисовать последовательно все предложения\n",
    "\n",
    "Для затравки вот картинка с арборатора:\n",
    "\n",
    "<img src='arbo.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717be5ab-3962-4e0f-b841-4bc64784471b",
   "metadata": {},
   "source": [
    "Авторы UD разместили онлайн-парсер на своем [сайте](https://lindat.mff.cuni.cz/services/udpipe/), однако для русских он без VPN не работает теперь, к сожалению."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc393b-a2fe-408d-a5de-cf46c4449e5e",
   "metadata": {},
   "source": [
    "## Крупные библиотеки NLP: SpaCy, Stanza (StanfordNLP), Natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64cb65-7964-434e-88d3-ba8b5e79067d",
   "metadata": {},
   "source": [
    "#### SpaCy\n",
    "\n",
    "Спейси - современная библиотека, которая написана в Cython и использует нейронные сети. Интерфейс спейси довольно удобный и однообразный. Центральное понятие для спейси - это pipeline: то есть, набор действий, которые спейси совершает с текстом. Чтобы обработать текст на любом из языков, представленных в библиотеке ([список](https://spacy.io/usage/models)), достаточно завести пустой пайплайн:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bfea6cb-4add-4fe9-8f76-4105aa764405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank('en')\n",
    "doc = nlp('My beautiful sentence is here.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e8ffb-a910-4315-90b4-0524c253576e",
   "metadata": {},
   "source": [
    "Предупреждения может выдавать библиотека tensorflow, которая сообщает, что у вас не настроена CUDA, но их можно игнорировать, как и предлагается. \n",
    "\n",
    "Пустой пайплайн превращает наш текст в особый объект, в котором текст разделен на токены, и можно у этих токенов смотреть самые простые характеристики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582cb28e-71d8-48ad-93e9-f5009c65763e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "№ 0. Токен:              My. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 1. Токен:       beautiful. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 2. Токен:        sentence. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 3. Токен:              is. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 4. Токен:            here. Является словом: True. Является пунктуацией: False. Похож на число: False\n",
      "№ 5. Токен:               .. Является словом: False. Является пунктуацией: True. Похож на число: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'№ {token.i}. Токен: {token.text:>15}. Является словом: {token.is_alpha}. Является пунктуацией: {token.is_punct}. Похож на число: {token.like_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aaec63-172b-494f-b19b-10588c72bb1d",
   "metadata": {},
   "source": [
    "Объект doc также позволяет смотреть спаны (несколько токенов, срез текста):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32bcaae-e97f-42e8-879e-a6f485e00592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful sentence'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span = doc[1:3]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c44c416-a851-4e6c-822f-18043cc19b45",
   "metadata": {},
   "source": [
    "Ну, это все прекрасно, но хотелось бы чего-то большего. Для spacy есть довольно много предобученных моделей для разных языков (список см. выше). Модельки нужно устанавливать (скачивать) с помощью команды в командной строке - она написана у них на сайте (python -m spacy download имя_модели). Когда вы загрузили модельку, вы можете ее использовать в коде:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b11337fb-8c1c-491f-a2b7-1805a5c15020",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444ea3a-dfb3-45ef-9127-ee90d45cabd4",
   "metadata": {},
   "source": [
    "Теперь уже можно получить сведения поинтереснее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e13d487-a2d8-4215-a3a8-cd1558f48881",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dee4392-51c6-46e3-9d34-3f2dc21b566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. The             POS: DET    SyntR: det       Head: Army\n",
      " 1. 4th             POS: ADJ    SyntR: amod      Head: Army\n",
      " 2. Army            POS: PROPN  SyntR: nsubj     Head: was\n",
      " 3. was             POS: AUX    SyntR: ROOT      Head: was\n",
      " 4. a               POS: DET    SyntR: det       Head: formation\n",
      " 5. Royal           POS: PROPN  SyntR: compound  Head: Army\n",
      " 6. Yugoslav        POS: ADJ    SyntR: amod      Head: Army\n",
      " 7. Army            POS: PROPN  SyntR: compound  Head: formation\n",
      " 8. formation       POS: NOUN   SyntR: attr      Head: was\n",
      " 9. mobilised       POS: VERB   SyntR: acl       Head: formation\n",
      "10. prior           POS: ADV    SyntR: advmod    Head: mobilised\n",
      "11. to              POS: ADP    SyntR: prep      Head: prior\n",
      "12. the             POS: DET    SyntR: det       Head: invasion\n",
      "13. German          POS: PROPN  SyntR: npadvmod  Head: led\n",
      "14. -               POS: PUNCT  SyntR: punct     Head: led\n",
      "15. led             POS: VERB   SyntR: amod      Head: invasion\n",
      "16. invasion        POS: NOUN   SyntR: pobj      Head: to\n",
      "17. of              POS: ADP    SyntR: prep      Head: invasion\n",
      "18. the             POS: DET    SyntR: det       Head: Kingdom\n",
      "19. Kingdom         POS: PROPN  SyntR: pobj      Head: of\n",
      "20. of              POS: ADP    SyntR: prep      Head: Kingdom\n",
      "21. Yugoslavia      POS: PROPN  SyntR: pobj      Head: of\n",
      "22. during          POS: ADP    SyntR: prep      Head: mobilised\n",
      "23. World           POS: PROPN  SyntR: compound  Head: II\n",
      "24. War             POS: PROPN  SyntR: compound  Head: II\n",
      "25. II              POS: PROPN  SyntR: pobj      Head: during\n",
      "26. .               POS: PUNCT  SyntR: punct     Head: was\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f'{token.i:2}. {token.text:15} POS: {token.pos_:6} SyntR: {token.dep_:9} Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c348220-34fc-44bf-aada-4b1e191d948e",
   "metadata": {},
   "source": [
    "Грамматические характеристики можно тоже посмотреть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3383206-ec87-4073-a269-fbde911ab532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definite=Def|PronType=Art\n",
      "Degree=Pos\n",
      "Number=Sing\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:3]:\n",
    "    print(token.morph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4537da-ac8f-4c13-8bba-56c9dffc80d2",
   "metadata": {},
   "source": [
    "Также spacy позволяет разметить именованные сущности и посмотреть, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a29b3fe-8624-4d9f-92e7-ca8d6e5376cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: The 4th Army                   Label: ORG\n",
      "Entity: Royal Yugoslav Army            Label: ORG\n",
      "Entity: German                         Label: NORP\n",
      "Entity: the Kingdom of Yugoslavia      Label: GPE\n",
      "Entity: World War II                   Label: EVENT\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f'Entity: {ent.text:30} Label: {ent.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced734d0-fda5-4c88-bfb2-0e32cc7d4db5",
   "metadata": {},
   "source": [
    "Если аббревиатуры вас смущают, spacy легко предоставит расшифровки (для всех!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcf491f3-09e0-4204-9f9c-a91bb3b488a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9b43579-11f5-453a-a098-74f884b4145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object of preposition'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('pobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a836c553-32c8-442f-a599-d4ae561170cf",
   "metadata": {},
   "source": [
    "Заметим, что спейси по умолчанию не делит текст на предложения, и нумерация токенов в нем сквозная. Если мы хотим это поведение изменить, то можно использовать следующий инструмент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482ddf11-3a9e-4b0d-8275-b4ba46470506",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('ru_core_news_sm')\n",
    "nlp.add_pipe('sentencizer') # добавим шаг с сентенизацией в пайплайн\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents: # теперь у нас есть атрибут sents\n",
    "  for token in sent:\n",
    "      # вершина предложения должна быть равна 0\n",
    "    if token.dep_ == 'ROOT':\n",
    "      head = 0\n",
    "    else:\n",
    "      head = token.head.i - sent.start + 1 # отсчитываем относительную вершину\n",
    "    # и относительный индекс для токена\n",
    "    print(token.i - sent.start + 1, token.text, head, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda40887-1115-4308-84a0-76c3b3f01797",
   "metadata": {},
   "source": [
    "Итак, а теперь самое интересное - это обертки для spacy + udpipe (для тех, кто хочет запускать его локально, а не с сайта). UD, как известно - очень популярный и влиятельный проект (чешский), цель которого - унифицировать тагсет для всех языков мира, причем как в морфологии, так и в синтаксисе. Неудивительно, что, поскольку в UD существует большое количество размеченных датасетов (русским языком активно занималась О. Ляшевская), то они обучили и свой парсер. Сам парсер написан в плюсах и не очень дружелюбен, но обертки для spacy делают жизнь проще. \n",
    "\n",
    "Внимание: я обычно это забываю, но прежде чем установить udpipe, **нужно поставить C++ Build Tools** (с сайта майкрософт, это бесплатно). Это нужно, чтобы скомпилировать udpipe из исходников в сях. Адрес сайта, откуда брать их, сам pip обычно подсказывает, но в целом можно просто погуглить. \n",
    "\n",
    "Две оболочки для udpipe, которые мы рассмотрим - это spacy_udpipe и corpy. Обе нужно устанавливать:\n",
    "\n",
    "    pip install spacy_udpipe\n",
    "    pip install corpy\n",
    "    \n",
    "Для spacy_udpipe вообще ничего больше не нужно, там максимально автоматизированно скачиваются модельки. Для corpy приходится скачивать искомую модель руками, [отсюда](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131) (к сожалению, с некоторых пор тоже открывается только с VPN). Эту модель вы можете положить куда угодно, главное потом указать corpy путь к ней. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d905811f-1d84-44f3-ae50-7113697f97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded a model for the 'en' language\n"
     ]
    }
   ],
   "source": [
    "import spacy_udpipe\n",
    "\n",
    "spacy_udpipe.download('en')  # эту команду достаточно выполнить только один раз - она как nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a54da47a-f568-4489-951a-cb6cd37d6a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0. The             Lemma: the             POS: DET    SyntR: det       Head: Army\n",
      " 1. 4th             Lemma: 4th             POS: ADJ    SyntR: amod      Head: Army\n",
      " 2. Army            Lemma: Army            POS: PROPN  SyntR: nsubj     Head: formation\n",
      " 3. was             Lemma: be              POS: AUX    SyntR: cop       Head: formation\n",
      " 4. a               Lemma: a               POS: DET    SyntR: det       Head: formation\n",
      " 5. Royal           Lemma: Royal           POS: PROPN  SyntR: compound  Head: Army\n",
      " 6. Yugoslav        Lemma: Yugoslav        POS: PROPN  SyntR: compound  Head: Army\n",
      " 7. Army            Lemma: Army            POS: PROPN  SyntR: compound  Head: formation\n",
      " 8. formation       Lemma: formation       POS: NOUN   SyntR: nsubj     Head: mobilised\n",
      " 9. mobilised       Lemma: mobilise        POS: VERB   SyntR: ROOT      Head: mobilised\n",
      "10. prior           Lemma: prior           POS: ADJ    SyntR: case      Head: invasion\n",
      "11. to              Lemma: to              POS: ADP    SyntR: fixed     Head: prior\n",
      "12. the             Lemma: the             POS: DET    SyntR: det       Head: invasion\n",
      "13. German          Lemma: german          POS: ADJ    SyntR: obl:npmod Head: led\n",
      "14. -               Lemma: -               POS: PUNCT  SyntR: punct     Head: led\n",
      "15. led             Lemma: lead            POS: VERB   SyntR: amod      Head: invasion\n",
      "16. invasion        Lemma: invasion        POS: NOUN   SyntR: obl       Head: mobilised\n",
      "17. of              Lemma: of              POS: ADP    SyntR: case      Head: Kingdom\n",
      "18. the             Lemma: the             POS: DET    SyntR: det       Head: Kingdom\n",
      "19. Kingdom         Lemma: Kingdom         POS: PROPN  SyntR: nmod      Head: invasion\n",
      "20. of              Lemma: of              POS: ADP    SyntR: case      Head: Yugoslavia\n",
      "21. Yugoslavia      Lemma: Yugoslavia      POS: PROPN  SyntR: nmod      Head: Kingdom\n",
      "22. during          Lemma: during          POS: ADP    SyntR: case      Head: II\n",
      "23. World           Lemma: World           POS: PROPN  SyntR: compound  Head: War\n",
      "24. War             Lemma: War             POS: PROPN  SyntR: compound  Head: II\n",
      "25. II              Lemma: ii              POS: PROPN  SyntR: nmod      Head: Kingdom\n",
      "26. .               Lemma: .               POS: PUNCT  SyntR: punct     Head: mobilised\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy_udpipe.load('en')\n",
    "doc = nlp('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')\n",
    "for token in doc:\n",
    "    print(f'{token.i:2}. {token.text:15} Lemma: {token.lemma_:15} POS: {token.pos_:6} SyntR: {token.dep_:9} Head: {token.head.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e17ec90-8c00-4c03-adce-793d9da12f6e",
   "metadata": {},
   "source": [
    "То же самое можно сделать в corpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "36008f0c-749f-4d5b-9715-f12c49313434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpy.udpipe import Model\n",
    "\n",
    "model = Model('english-partut-ud-2.5-191206.udpipe')  \n",
    "# тут и нужно указать путь к вашей модели. Если она лежит в той же папке, что и скрипт, достаточно только имени файла\n",
    "\n",
    "sents = model.process('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebab3b5-16bd-498b-81c9-9846776cf5d1",
   "metadata": {},
   "source": [
    "corpy возвращает генератор (то есть, итерируемый объект, который как магазин автомата, расстреляли все патроны - он опустел; повторно по генератору итерироваться нельзя). Генератор на каждом шаге выдает предложение (объект специального класса Sentence()), а в предложении - объекты класса Word(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4897965f-9d04-4d9b-bfa3-b4c210f00993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<root>          Лемма: <root>          POS: <root> Грам. инфа: <root>\n",
      "The             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "4th             Лемма: 4th             POS: ADJ Грам. инфа: Degree=Pos\n",
      "Army            Лемма: army            POS: NOUN Грам. инфа: Number=Sing\n",
      "was             Лемма: be              POS: AUX Грам. инфа: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "a               Лемма: a               POS: DET Грам. инфа: Definite=Ind|Number=Sing|PronType=Art\n",
      "Royal           Лемма: royal           POS: PROPN Грам. инфа: \n",
      "Yugoslav        Лемма: Yugoslav        POS: PROPN Грам. инфа: \n",
      "Army            Лемма: army            POS: PROPN Грам. инфа: \n",
      "formation       Лемма: formation       POS: NOUN Грам. инфа: Number=Sing\n",
      "mobilised       Лемма: mobilize        POS: VERB Грам. инфа: Mood=Ind|Person=3|Tense=Past|VerbForm=Fin\n",
      "prior           Лемма: prior           POS: ADJ Грам. инфа: Degree=Pos\n",
      "to              Лемма: to              POS: ADP Грам. инфа: \n",
      "the             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "German          Лемма: German          POS: PROPN Грам. инфа: \n",
      "-               Лемма: -               POS: PUNCT Грам. инфа: \n",
      "led             Лемма: lead            POS: VERB Грам. инфа: Tense=Past|VerbForm=Part\n",
      "invasion        Лемма: invasion        POS: NOUN Грам. инфа: Number=Sing\n",
      "of              Лемма: of              POS: ADP Грам. инфа: \n",
      "the             Лемма: the             POS: DET Грам. инфа: Definite=Def|PronType=Art\n",
      "Kingdom         Лемма: kingdom         POS: NOUN Грам. инфа: Number=Sing\n",
      "of              Лемма: of              POS: ADP Грам. инфа: \n",
      "Yugoslavia      Лемма: Yugoslavia      POS: PROPN Грам. инфа: \n",
      "during          Лемма: during          POS: ADP Грам. инфа: \n",
      "World           Лемма: world           POS: NOUN Грам. инфа: Number=Sing\n",
      "War             Лемма: war             POS: NOUN Грам. инфа: Number=Sing\n",
      "II              Лемма: second          POS: ADJ Грам. инфа: Degree=Pos|NumType=Ord\n",
      ".               Лемма: .               POS: PUNCT Грам. инфа: \n",
      "Алилуя!\n"
     ]
    }
   ],
   "source": [
    "for sent in sents:\n",
    "    for word in sent.words:\n",
    "        print(f'{word.form:15} Лемма: {word.lemma:15} POS: {word.upostag} Грам. инфа: {word.feats}')\n",
    "print('Алилуя!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d9c0c-0261-42b8-9f52-bf1af5dfca1e",
   "metadata": {},
   "source": [
    "у corpy, кстати, есть свой способ вывода имеющейся информации (хотя, по-моему, в юпитере и так красиво выводится...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a0bfc86-0d79-47a3-aff6-cd0061f9597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sentence(\n",
      "   comments=[\n",
      "     '# newdoc',\n",
      "     '# newpar',\n",
      "     '# sent_id = 1',\n",
      "     '# text = The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.'],\n",
      "   words=[\n",
      "     Word(id=0, <root>),\n",
      "     Word(id=1,\n",
      "          form='The',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=3,\n",
      "          deprel='det'),\n",
      "     Word(id=2,\n",
      "          form='4th',\n",
      "          lemma='4th',\n",
      "          xpostag='A',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos',\n",
      "          head=3,\n",
      "          deprel='amod'),\n",
      "     Word(id=3,\n",
      "          form='Army',\n",
      "          lemma='army',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=9,\n",
      "          deprel='nsubj'),\n",
      "     Word(id=4,\n",
      "          form='was',\n",
      "          lemma='be',\n",
      "          xpostag='V',\n",
      "          upostag='AUX',\n",
      "          feats='Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin',\n",
      "          head=9,\n",
      "          deprel='cop'),\n",
      "     Word(id=5,\n",
      "          form='a',\n",
      "          lemma='a',\n",
      "          xpostag='RI',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Ind|Number=Sing|PronType=Art',\n",
      "          head=9,\n",
      "          deprel='det'),\n",
      "     Word(id=6,\n",
      "          form='Royal',\n",
      "          lemma='royal',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=9,\n",
      "          deprel='nmod'),\n",
      "     Word(id=7,\n",
      "          form='Yugoslav',\n",
      "          lemma='Yugoslav',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=6,\n",
      "          deprel='flat'),\n",
      "     Word(id=8,\n",
      "          form='Army',\n",
      "          lemma='army',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=6,\n",
      "          deprel='flat'),\n",
      "     Word(id=9,\n",
      "          form='formation',\n",
      "          lemma='formation',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='nsubj'),\n",
      "     Word(id=10,\n",
      "          form='mobilised',\n",
      "          lemma='mobilize',\n",
      "          xpostag='V',\n",
      "          upostag='VERB',\n",
      "          feats='Mood=Ind|Person=3|Tense=Past|VerbForm=Fin',\n",
      "          head=0,\n",
      "          deprel='root'),\n",
      "     Word(id=11,\n",
      "          form='prior',\n",
      "          lemma='prior',\n",
      "          xpostag='A',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos',\n",
      "          head=10,\n",
      "          deprel='amod'),\n",
      "     Word(id=12,\n",
      "          form='to',\n",
      "          lemma='to',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=17,\n",
      "          deprel='case'),\n",
      "     Word(id=13,\n",
      "          form='the',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=17,\n",
      "          deprel='det'),\n",
      "     Word(id=14,\n",
      "          form='German',\n",
      "          lemma='German',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=16,\n",
      "          deprel='obl',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=15,\n",
      "          form='-',\n",
      "          lemma='-',\n",
      "          xpostag='FF',\n",
      "          upostag='PUNCT',\n",
      "          head=16,\n",
      "          deprel='punct',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=16,\n",
      "          form='led',\n",
      "          lemma='lead',\n",
      "          xpostag='V',\n",
      "          upostag='VERB',\n",
      "          feats='Tense=Past|VerbForm=Part',\n",
      "          head=17,\n",
      "          deprel='acl'),\n",
      "     Word(id=17,\n",
      "          form='invasion',\n",
      "          lemma='invasion',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='obl'),\n",
      "     Word(id=18,\n",
      "          form='of',\n",
      "          lemma='of',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=20,\n",
      "          deprel='case'),\n",
      "     Word(id=19,\n",
      "          form='the',\n",
      "          lemma='the',\n",
      "          xpostag='RD',\n",
      "          upostag='DET',\n",
      "          feats='Definite=Def|PronType=Art',\n",
      "          head=20,\n",
      "          deprel='det'),\n",
      "     Word(id=20,\n",
      "          form='Kingdom',\n",
      "          lemma='kingdom',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=17,\n",
      "          deprel='nmod'),\n",
      "     Word(id=21,\n",
      "          form='of',\n",
      "          lemma='of',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=22,\n",
      "          deprel='case'),\n",
      "     Word(id=22,\n",
      "          form='Yugoslavia',\n",
      "          lemma='Yugoslavia',\n",
      "          xpostag='SP',\n",
      "          upostag='PROPN',\n",
      "          head=20,\n",
      "          deprel='nmod'),\n",
      "     Word(id=23,\n",
      "          form='during',\n",
      "          lemma='during',\n",
      "          xpostag='E',\n",
      "          upostag='ADP',\n",
      "          head=25,\n",
      "          deprel='case'),\n",
      "     Word(id=24,\n",
      "          form='World',\n",
      "          lemma='world',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=25,\n",
      "          deprel='nmod'),\n",
      "     Word(id=25,\n",
      "          form='War',\n",
      "          lemma='war',\n",
      "          xpostag='S',\n",
      "          upostag='NOUN',\n",
      "          feats='Number=Sing',\n",
      "          head=10,\n",
      "          deprel='obl'),\n",
      "     Word(id=26,\n",
      "          form='II',\n",
      "          lemma='second',\n",
      "          xpostag='NO',\n",
      "          upostag='ADJ',\n",
      "          feats='Degree=Pos|NumType=Ord',\n",
      "          head=25,\n",
      "          deprel='amod',\n",
      "          misc='SpaceAfter=No'),\n",
      "     Word(id=27,\n",
      "          form='.',\n",
      "          lemma='.',\n",
      "          xpostag='FS',\n",
      "          upostag='PUNCT',\n",
      "          head=10,\n",
      "          deprel='punct',\n",
      "          misc='SpaceAfter=No')])]\n"
     ]
    }
   ],
   "source": [
    "from corpy.udpipe import pprint\n",
    "\n",
    "pprint(list(model.process('The 4th Army was a Royal Yugoslav Army formation mobilised prior to the German-led invasion of the Kingdom of Yugoslavia during World War II.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854acd66-7707-4940-8650-27218e10b159",
   "metadata": {
    "id": "5GU9unbnvJXP"
   },
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61fe04f-b598-4166-9eaa-fa86b33109c1",
   "metadata": {
    "id": "eKwCF6UOvRCh"
   },
   "outputs": [],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd813c9-08af-4629-bbc6-0d323970c9c2",
   "metadata": {
    "id": "Zv7BrtxEvUwY"
   },
   "source": [
    "Загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea946fa-3931-4ed6-b650-832e2bcb0544",
   "metadata": {
    "executionInfo": {
     "elapsed": 2995,
     "status": "ok",
     "timestamp": 1663154625797,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "8UtSgSntvXro"
   },
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a834d5-1dee-4bee-a14d-341a483079d5",
   "metadata": {
    "id": "P7VKLnRkvZd4"
   },
   "outputs": [],
   "source": [
    "nlp_ru = stanza.Pipeline(lang='ru')\n",
    "nlp_en = stanza.Pipeline(lang='en', processors='tokenize, pos, constituency')\n",
    "nlp_fr = stanza.Pipeline(lang='fr', processors='tokenize, mwt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1577e0f-a71f-48bc-936c-12d36577d661",
   "metadata": {
    "id": "dUR7LXjmvtpn"
   },
   "source": [
    "Токенизация, сегментация по предложениям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ca0e48e-5063-4762-9ef2-bfd90ec37a0e",
   "metadata": {
    "executionInfo": {
     "elapsed": 2242,
     "status": "ok",
     "timestamp": 1663154958031,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "Db8CYc_SvtIX"
   },
   "outputs": [],
   "source": [
    "text = '''Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века. В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома. Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.'''\n",
    "\n",
    "doc = nlp_ru(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c15f36b0-0422-4951-b866-f9ef11a9b07e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 273,
     "status": "ok",
     "timestamp": 1663154817857,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "MMT0yrtGv3wP",
    "outputId": "2e4e3a15-9345-47c7-ed71-e4c8fc17dc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Архитектура Византии — совокупность традиций строительства и архитектуры в поздней Римской империи и в Византии в период с начала IV века по середину XV века.\n",
      "В качестве отдельных направлений исследования выделяют религиозную архитектуру Византии, византийскую фортификацию и гражданское строительство, включающее дворцы, общественные сооружения и частные дома.\n",
      "Также в рамках данной дисциплины изучают традиции строительного ремесла и декоративного искусства.\n"
     ]
    }
   ],
   "source": [
    "print(*[sentence.text for sentence in doc.sentences], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b89e6-46ff-4b29-b7f0-a01010b85153",
   "metadata": {
    "id": "IxBb4XOMwJdf"
   },
   "outputs": [],
   "source": [
    "for i, sentence in enumerate(doc.sentences):\n",
    "  print(f'====== Sentence {i + 1} tokens =======')\n",
    "  print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d4b73-2763-4d6c-b665-8bcd678977e0",
   "metadata": {
    "id": "i2cRJqFSwO-v"
   },
   "outputs": [],
   "source": [
    "text_fr = '''Il est révélé par les romans Extension du domaine de la lutte (1994) et, surtout, Les Particules élémentaires (1998), qui le fait connaître d'un large public.'''\n",
    "\n",
    "doc_fr = nlp_fr(text_fr)\n",
    "for token in doc_fr.sentences[0].tokens:\n",
    "    print(f'token: {token.text}\\twords: {\", \".join([word.text for word in token.words])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f9630-02e6-49fb-90ab-d802c42e399e",
   "metadata": {
    "id": "HlS7SKLUwetI"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08b822-b0f5-46cd-afbe-57196f378972",
   "metadata": {
    "id": "Fh6HPJn2wgcp"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad28175-cba0-4ef3-8fea-05961d1100aa",
   "metadata": {
    "id": "5SsW-5MFwupQ"
   },
   "source": [
    "Морфопарсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc9937-fd76-4bac-8d53-fb54c94db382",
   "metadata": {
    "id": "Z2wxiB8NwtYf"
   },
   "outputs": [],
   "source": [
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192211aa-fb4b-400c-ab7d-69be479a02bf",
   "metadata": {
    "id": "bKZwX5l8wy6Q"
   },
   "source": [
    "Парсинг синтаксических зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594076c8-5c20-401c-b370-4f635eb44de5",
   "metadata": {
    "id": "w11Jqo6Fwx14"
   },
   "outputs": [],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89b39e3-1d64-4608-890e-46e616f2d3ff",
   "metadata": {
    "id": "k7zTW3sJw27A"
   },
   "outputs": [],
   "source": [
    "doc.sentences[0].print_dependencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bed59-9425-4a05-a8d0-177051eae360",
   "metadata": {
    "id": "8-1cXBtWw5K4"
   },
   "source": [
    "Парсинг составляющих (для русского недоступен)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c55c55-451d-451e-91a4-9b74787d5289",
   "metadata": {
    "id": "5KGb8sV6w6lo"
   },
   "outputs": [],
   "source": [
    "doc_en = nlp_en('This is a sentence for parsing constituencies.')\n",
    "\n",
    "for sentence in doc_en.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eab694b-ac41-49dc-8257-b32fe90fb820",
   "metadata": {
    "id": "HWfmZei5t_8I"
   },
   "source": [
    "### natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5c19b-ef93-4678-a58d-ea5eb471587b",
   "metadata": {
    "id": "Fw52LhAct9Tg"
   },
   "outputs": [],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7ea18-d19c-47c4-81e5-c9a61900183b",
   "metadata": {
    "id": "8VqX_VxouI-q"
   },
   "source": [
    "Морфосинтаксический парсинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8840aefc-f068-4619-b8d8-d6934c7a7b49",
   "metadata": {
    "executionInfo": {
     "elapsed": 253,
     "status": "ok",
     "timestamp": 1663154496060,
     "user": {
      "displayName": "Alexandra Ivoylova",
      "userId": "06069991353189522669"
     },
     "user_tz": -180
    },
    "id": "S5k9Wlz9uK7M"
   },
   "outputs": [],
   "source": [
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    \n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53711c4c-1e90-40fe-ac85-d94238db9ba2",
   "metadata": {
    "id": "ciOkGldHuLgB"
   },
   "outputs": [],
   "source": [
    "segmenter = Segmenter()  # токенизация и разделение на предложения\n",
    "emb = NewsEmbedding()  # эмбеддинги\n",
    "morph_tagger = NewsMorphTagger(emb)  # морфология\n",
    "syntax_parser = NewsSyntaxParser(emb) # синтаксис\n",
    "\n",
    "text = '29 марта 2017 года правительство Великобритании инициировало процедуру выхода в соответствии со статьёй 50 Договора о Европейском союзе; первоначально планировалось, что Великобритания покинет Европейский союз через два года, 29 марта 2019 года в 23:00 по Гринвичу.'\n",
    "doc = Doc(text)\n",
    "\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "sent = doc.sents[0]\n",
    "sent.morph.print()\n",
    "sent.syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e1430-c0ba-444f-a7c8-6271a13c8b14",
   "metadata": {
    "id": "YPCVWdB1ujFp"
   },
   "source": [
    "Распознание именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8531922-b36a-491a-9d3b-53d4c690b4f5",
   "metadata": {
    "id": "AaTUTjyMuikI"
   },
   "outputs": [],
   "source": [
    "from natasha import NewsNERTagger\n",
    "\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "doc.tag_ner(ner_tagger)\n",
    "doc.ner.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b565d-2267-4158-975e-40d411e2139e",
   "metadata": {
    "id": "Log6uopyux7n"
   },
   "source": [
    "Лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601683b-9191-48ee-a823-9c5b858a1eb0",
   "metadata": {
    "id": "RNSGZViPuuJd"
   },
   "outputs": [],
   "source": [
    "from natasha import MorphVocab\n",
    "\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "for token in doc.tokens:\n",
    "  token.lemmatize(morph_vocab)\n",
    "  print(token.lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e37179-c44b-40b3-b357-3a805b41d06b",
   "metadata": {
    "id": "o1dqpS_Hu3OT"
   },
   "source": [
    "Нормализация именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee6916e-5fa9-4b7a-b28f-3ffb9c202e7c",
   "metadata": {
    "id": "gqmxRFqbu2m8"
   },
   "outputs": [],
   "source": [
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "   \n",
    "{_.text: _.normal for _ in doc.spans}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
